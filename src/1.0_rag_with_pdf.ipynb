{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2b7492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126e9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if os.environ['OPENAI_API_KEY']:\n",
    "    print(\"API Key is set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bd5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efc46348",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d87c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83b3d5eb",
   "metadata": {},
   "source": [
    "### RAG implementation with pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de529205",
   "metadata": {},
   "source": [
    "### Extracting text from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d21bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d2d5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content=\"1 \\n \\n \\nEnhancing Natural Language Understanding in \\nCompact LLMs via Task-Specific Knowledge \\nDistillation  \\n \\nAbstract \\n \\nLarge language models (LLMs) have demonstrated outstanding \\nperformance across a variety of natural language processing applications.  \\nNatural Language Understanding (NLU) is a key characteristic that allows \\nLLMs to interpret language, reasoning around context, and provide correct \\nand coherent replies.  Although proprietary LLMs exhibit strong NLU \\nbehavior, their computing requirements hinder practical application.  \\nCompact models such as LLaMA 3.1: 8B and Mistral 7B provide a more \\nefficient substitute, however these models  frequently fail to preserve \\nsemantic correctness, reasoning consistency, and reliability across tasks.  \\nThis study proposes a knowledge distillation-based approach for improving \\nthe NLU skills of compact LLMs by transferring information from a high -\\ncapacity teacher model.  Distillation is performed on three tasks that reflect \\nimportant aspects of NLU: text s ummarization, text classification, and \\nsentiment analysis. All experiments are carried out using LLaMA models, \\nwith LLaMA 3.1:70B serving as the teacher and LLaMA  3.1:8B as the \\nstudent.  We use both hard and soft labels to teach a student model.  In text \\nsummarization, the distilled model obtains a 0.85 STS-B Pearson score and \\n0.81 MNLI accuracy, keeping more than 90% of the teacher's performance \\nwhile lowering latency from 12s to 3s, and producing summaries that are \\nmore semantically consistent and closer to the original meaning.  Distillation \\nincreases student accuracy in sentiment analysis from 0.4025 to 0.5900, \\nmaking the model more sensitive to subtle emotional signals and context -\\ndependent polarity shifts. Text classification accuracy improves from 31.0% \\nto 48.0%, leading to better distinctions between the three news categories \\nand more stable domain-aware decision bounds.  These findings suggest that \\ntask-specific distillation might significantly improve th e NLU capabilities \\nof small LLMs, allowing for economical and reliable deployment in \\nresource-constrained situations. \\n \\nKey words: Large Language Models; Knowledge Distillation; Natural \\nLanguage Understanding; Semantic; Reasoning.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"1 \\n \\n \\nIntroduction \\n \\nNatural Language Understanding (NLU) is a fundamental capability of LLMs, allowing them \\nto process meaning, observe semantic relationships, and reason about context.  While large \\nmodels are proficient at these tasks, their high computing cost and latency make practical \\ndeployment difficult. Compact LLMs are efficient but frequently suffer with semantic accuracy \\nand consistent decision -making (Gou et al., 2021) . Knowledge distillation (KD) provides a \\npromising solution by transferring the semantic, reasoning, and interpretive behaviors of a large \\nteacher model to a lightweight student model (Gou et al., 2021) (Balasubramaniam et al., 2026). \\nThis work examines how task-specific distillation improves NLU in compact LLMs across \\nsummarization, sentiment analysis, and classification tasks. By aligning the student’s output \\ndistribution with the teacher’s output distribution and separately evaluating teacher, student, \\nand distilled models, we  evaluate the improvements in semantic alignment, contextual \\nreasoning, and predicti on reliability. Overall, the results show that task -specific knowledge \\ndistillation improves the NLU capabilities of compact LLMs while lowering computing needs. \\n \\nLiterature Review \\n \\nLarge Language Models have demonstrated exceptional performance in tasks such as text \\nsummarization, sentiment analysis, and text classification.  Transformer-based models such as \\nBERT, GPT, and T5, are more successful at capturing long -term relationships and contextual \\nmeaning than recurrent or convolutional networks.  While large models like GPT -4 and \\nLLaMA-70B produce impressive results, their computational and memory requirements \\nrestrict their practical application, driving studies on model compression and efficiency. \\n \\nKnowledge Distillation (KD) has evolved as an effective approach for transferring knowledge \\nfrom massive instructor models to smaller, more efficient student models  (Gou et al., 2021) .  \\nTask-agnostic KD strategies like as DistilBERT  (Sanh et al., 2019) , TinyBERT (Jiao et al., \\n2019), and MobileBERT (Sun et al., 2020) , aim to preserve the teacher's general skills while \\nproviding compact models appropriate for a wide range of NLP applications and decreasing \\nparameters and inference time.  In contrast, task -specific KD focuses on individual tasks to \\nimprove student models for precise knowledge transfer.  Previous studies employed task -\\nspecific distillation to improve task performance in summarization, sentiment analysis, and text \\nclassification, but majority of them focused on speed or compression rather than increasing \\ncore language understanding in small models  (Gou et al., 2021) . This study fills that gap by \\nemploying task -specific distillation in summarization, sentiment analysis, and text \\ncategorization to transmit semantic, contextual, and reasoning information from big instructor \\nmodels to smaller student models. \\n \\nMethodology \\n \\nFigure 1 shows the research's overall workflow, which consists of four stages : task \\nidentification and dataset preparation, knowledge elicitation, skill distillation, and evaluation.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content=\"2 \\n \\n \\n \\n \\n \\n \\nThe present study employs a structured task -specific distillation process to perform three \\nexemplary NLU tasks: text summarization, sentiment analysis, and multiclass text \\nclassification.  For summarization, we constructed a 300 -paragraph dataset comprising \\nstorybooks, online articles, AI -generated texts, ne wspapers, and scientific publications.  \\nSentiment analysis employs a real -world movie review dataset with balanced positive and \\nnegative samples. Text classification is based on a three-category news dataset encompassing \\nentertainment, politics, and sports . All datasets are normalized, deduplicated, tokenized, and \\nsequence-length controlled, and quality is verified via class balance checks, label consistency \\nverification, and manual sample inspection. \\n \\nKnowledge elicitation is using task -aligned prompting, seed -knowledge injection, and \\nextracting soft logits from the teacher model. The distillation process then refines the student \\nmodel by adjusting its output distribution to closely match that of the instructor, which is \\naccomplished via a combination of KL -divergence and cross -entropy loss.  To measure \\nimprovements in performance, independent evaluations of teacher, student, and distilled \\nmodels are conducted.  Accuracy, precision, recall, and F1 -score are used to grade sentiment \\nanalysis and text categorization, respectively, while summarization quality is examined using \\nthe STS-B and MNLI benchmark tasks. This methodology provides a consistent framework for \\ninvestigating how well task-specific distillation allows compact models to inherit and preserve \\nthe teacher's semantic reasoning and language comprehension abilities. \\n \\nResults and Discussion \\n \\nAs shown in Table 1 and Table 2, t he results demonstrates that task -specific knowledge \\ndistillation improves the compact LLMs' Natural Language Understanding skills on all three \\ntasks.  In text classification, the teacher achieves a solid baseline of 0.5534 accuracy and 0.5668 \\nF1 score, however the  student performs poorly at 0.3100 accuracy.  After distillation, the \\nFigure 1: High level methodology diagram\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"3 \\n \\n \\nstudent improves to 0.4800 accuracy and 0.4952 F1, indicating that the teacher logits assist the \\nmodel in learning discriminative patterns and reducing misclassification.  Similarly, in \\nsentiment analysis, the teacher reaches 0.6550 accuracy and 0.6318 F1,  whereas student \\ninitially achieves 0.4025 accuracy.  Distillation improves the student's accuracy and F1 to \\n0.5900 and 0.5628  respectively, indicating increased detection of subtle sentiment clues and \\nless polarity errors. \\n \\nTable 1: Text classification and sentiment analysis performance \\nTask Model Accuracy Precision Recall F1 Score \\nText \\nClassification \\nTeacher (LLaMA \\n3.1:70B) \\n0.5534 0.5111 0.5801 0.5668 \\nStudent (LLaMA \\n3.1:8B) \\n0.3100 0.2900 0.2101 0.3552 \\nDistilled 0.4800 0.4700 0.4300 0.4952 \\nSentiment \\nAnalysis \\nTeacher (LLaMA \\n3.1:70B) \\n0.6550 0.6210 0.6430 0.6318 \\nStudent (LLaMA \\n3.1:8B) \\n0.4025 0.3901 0.3620 0.3756 \\nDistilled 0.5900 0.5740 0.5520 0.5628 \\n \\n \\nTable 2: Text summarization task performance \\nModel STS-B (Pearson \\nCorr.) \\nMNLI Accuracy Average Response \\ntime (s) \\nTeacher (LLaMA \\n3.1:70B) \\n0.94 0.90 12.0 \\nStudent (LLaMA \\n3.1:8B) \\n0.58 0.65 3.2 \\nDistilled 0.85 0.81 3.0 \\n \\nThe teacher obtains 0.94 STS -B and 0.90 MNLI for text summarization, but the student \\nperforms much lower at 0.58 STS-B and 0.65 MNLI.  After distillation, the student advances \\nto 0.85 STS-B and 0.81 MNLI, recovering more than 90% of the teacher's performance.  The \\ndistilled model retains low inference latency, providing a 4x speedup and creating more truthful \\nand coherent summaries.  Overall, the findings show that task-specific logit-based knowledge \\ndistillation improves compact LLMs' semantic reasoning, class ification stability, and \\ncontextual comprehension.  The distilled model consistently outperforms the baseline student \\nacross all tasks and exhibits teacher -like NLU behavior while maintaining computational \\nefficiency, making it appropriate for real-world, resource-constrained implementation. \\n \\nConclusions \\n \\nThis study demonstrates that task -specific, logit -based knowledge distillation significantly\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content=\"4 \\n \\n \\nimproves NLU in compact LLMs.  By transferring soft -label predictions and task -specific \\nreasoning from LLaMA3.1:70B to LLaMA3.1:8B, the distilled model significantly  improves \\ntext classification, sentiment analysis, and summarization.  It captures deeper semantic links, \\ncontextual reasoning, and classification consistency, restoring much of the teacher's \\nperformance while minimizing inference delay.  These findings sho w that task -focused \\ndistillation allows compact models to efficiently accomplish teacher-like NLU behavior, hence \\npromoting high-quality language understanding in resource-constrained environments. \\n \\nAlthough the results are favorable, the study has a few drawbacks.  Due to limited computing \\npower, substantial fine -tuning, domain adaptation, and repeat experimental runs were not \\npossible.  The evaluation was based on a limited collection of datasets and benchmarks, which \\nmay not apply to larger domains, languages, or real-world settings.  Furthermore, the distillation \\nprocess only exploited logit -level transfer, leaving deeper internal reasoning processes \\nuntapped.  To increase its versatility and application, future work might broaden this framework \\nto include multi-class, multilingual, and cross-domain scenarios. Further improvements could \\nbe achieved by investigating deeper distillation approaches, including as layer -wise or \\nrepresentation-level trans fer, as well as fine -tuning the instructor for specific tasks prior to \\ndistillation.  Complementary studies on interpretability and error analysis might give more \\ninformation about how well the student model recalls and applies the teacher's reasoning. \\n \\nIn conclusion, this research demonstrates that task-specific, logit-based knowledge distillation \\nis a useful technique for improving NLU in compact LLMs.  By demonstrating consistent \\nperformance improvements across multiple tasks, the research indicates th e potential of \\ntargeted distillation approaches for  deploying high -performing, resource -efficient LLMs in \\nreal-world applications, providing the groundwork for future advances in compact NLU \\nmodels. \\n    \\nReferences \\n \\nBalasubramaniam, G., Abishethvarman, V., Kumara, B. T. G. S., Prasanth, S., & \\nKuhaneswaran, B. (2026). Task -Specific Knowledge Distillation for  Scalable \\nSentiment Classification in  Low-Resource Settings. In C. Anutariya, M. Bonsangue, \\nA. Pinidiyaarachchi, & H. Usoof, Data Science and Artificial Intelligence Singapore. \\nGou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). Knowledge distillation: A survey. \\nInternational Journal of Computer Vision, 129(6), 1789-1819.  \\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., & Liu, Q. (2019). Tinybert: \\nDistilling bert for natural language understanding. arXiv preprint arXiv:1909. 10351.  \\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of \\nBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.  \\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., & Zhou, D. (2020). Mobilebert: a compact task -\\nagnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984.\")]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./Docs/COMURS_GB_v4.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c4a55a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='1 \\n \\n \\nEnhancing Natural Language Understanding in \\nCompact LLMs via Task-Specific Knowledge \\nDistillation  \\n \\nAbstract \\n \\nLarge language models (LLMs) have demonstrated outstanding \\nperformance across a variety of natural language processing applications.  \\nNatural Language Understanding (NLU) is a key characteristic that allows \\nLLMs to interpret language, reasoning around context, and provide correct \\nand coherent replies.  Although proprietary LLMs exhibit strong NLU \\nbehavior, their computing requirements hinder practical application.  \\nCompact models such as LLaMA 3.1: 8B and Mistral 7B provide a more \\nefficient substitute, however these models  frequently fail to preserve \\nsemantic correctness, reasoning consistency, and reliability across tasks.  \\nThis study proposes a knowledge distillation-based approach for improving \\nthe NLU skills of compact LLMs by transferring information from a high -\\ncapacity teacher model.  Distillation is performed on three tasks that reflect'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content=\"capacity teacher model.  Distillation is performed on three tasks that reflect \\nimportant aspects of NLU: text s ummarization, text classification, and \\nsentiment analysis. All experiments are carried out using LLaMA models, \\nwith LLaMA 3.1:70B serving as the teacher and LLaMA  3.1:8B as the \\nstudent.  We use both hard and soft labels to teach a student model.  In text \\nsummarization, the distilled model obtains a 0.85 STS-B Pearson score and \\n0.81 MNLI accuracy, keeping more than 90% of the teacher's performance \\nwhile lowering latency from 12s to 3s, and producing summaries that are \\nmore semantically consistent and closer to the original meaning.  Distillation \\nincreases student accuracy in sentiment analysis from 0.4025 to 0.5900, \\nmaking the model more sensitive to subtle emotional signals and context -\\ndependent polarity shifts. Text classification accuracy improves from 31.0% \\nto 48.0%, leading to better distinctions between the three news categories\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='to 48.0%, leading to better distinctions between the three news categories \\nand more stable domain-aware decision bounds.  These findings suggest that \\ntask-specific distillation might significantly improve th e NLU capabilities \\nof small LLMs, allowing for economical and reliable deployment in \\nresource-constrained situations. \\n \\nKey words: Large Language Models; Knowledge Distillation; Natural \\nLanguage Understanding; Semantic; Reasoning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='1 \\n \\n \\nIntroduction \\n \\nNatural Language Understanding (NLU) is a fundamental capability of LLMs, allowing them \\nto process meaning, observe semantic relationships, and reason about context.  While large \\nmodels are proficient at these tasks, their high computing cost and latency make practical \\ndeployment difficult. Compact LLMs are efficient but frequently suffer with semantic accuracy \\nand consistent decision -making (Gou et al., 2021) . Knowledge distillation (KD) provides a \\npromising solution by transferring the semantic, reasoning, and interpretive behaviors of a large \\nteacher model to a lightweight student model (Gou et al., 2021) (Balasubramaniam et al., 2026). \\nThis work examines how task-specific distillation improves NLU in compact LLMs across \\nsummarization, sentiment analysis, and classification tasks. By aligning the student’s output \\ndistribution with the teacher’s output distribution and separately evaluating teacher, student,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='distribution with the teacher’s output distribution and separately evaluating teacher, student, \\nand distilled models, we  evaluate the improvements in semantic alignment, contextual \\nreasoning, and predicti on reliability. Overall, the results show that task -specific knowledge \\ndistillation improves the NLU capabilities of compact LLMs while lowering computing needs. \\n \\nLiterature Review \\n \\nLarge Language Models have demonstrated exceptional performance in tasks such as text \\nsummarization, sentiment analysis, and text classification.  Transformer-based models such as \\nBERT, GPT, and T5, are more successful at capturing long -term relationships and contextual \\nmeaning than recurrent or convolutional networks.  While large models like GPT -4 and \\nLLaMA-70B produce impressive results, their computational and memory requirements \\nrestrict their practical application, driving studies on model compression and efficiency.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"restrict their practical application, driving studies on model compression and efficiency. \\n \\nKnowledge Distillation (KD) has evolved as an effective approach for transferring knowledge \\nfrom massive instructor models to smaller, more efficient student models  (Gou et al., 2021) .  \\nTask-agnostic KD strategies like as DistilBERT  (Sanh et al., 2019) , TinyBERT (Jiao et al., \\n2019), and MobileBERT (Sun et al., 2020) , aim to preserve the teacher's general skills while \\nproviding compact models appropriate for a wide range of NLP applications and decreasing \\nparameters and inference time.  In contrast, task -specific KD focuses on individual tasks to \\nimprove student models for precise knowledge transfer.  Previous studies employed task -\\nspecific distillation to improve task performance in summarization, sentiment analysis, and text \\nclassification, but majority of them focused on speed or compression rather than increasing\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"classification, but majority of them focused on speed or compression rather than increasing \\ncore language understanding in small models  (Gou et al., 2021) . This study fills that gap by \\nemploying task -specific distillation in summarization, sentiment analysis, and text \\ncategorization to transmit semantic, contextual, and reasoning information from big instructor \\nmodels to smaller student models. \\n \\nMethodology \\n \\nFigure 1 shows the research's overall workflow, which consists of four stages : task \\nidentification and dataset preparation, knowledge elicitation, skill distillation, and evaluation.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='2 \\n \\n \\n \\n \\n \\n \\nThe present study employs a structured task -specific distillation process to perform three \\nexemplary NLU tasks: text summarization, sentiment analysis, and multiclass text \\nclassification.  For summarization, we constructed a 300 -paragraph dataset comprising \\nstorybooks, online articles, AI -generated texts, ne wspapers, and scientific publications.  \\nSentiment analysis employs a real -world movie review dataset with balanced positive and \\nnegative samples. Text classification is based on a three-category news dataset encompassing \\nentertainment, politics, and sports . All datasets are normalized, deduplicated, tokenized, and \\nsequence-length controlled, and quality is verified via class balance checks, label consistency \\nverification, and manual sample inspection. \\n \\nKnowledge elicitation is using task -aligned prompting, seed -knowledge injection, and \\nextracting soft logits from the teacher model. The distillation process then refines the student'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content=\"extracting soft logits from the teacher model. The distillation process then refines the student \\nmodel by adjusting its output distribution to closely match that of the instructor, which is \\naccomplished via a combination of KL -divergence and cross -entropy loss.  To measure \\nimprovements in performance, independent evaluations of teacher, student, and distilled \\nmodels are conducted.  Accuracy, precision, recall, and F1 -score are used to grade sentiment \\nanalysis and text categorization, respectively, while summarization quality is examined using \\nthe STS-B and MNLI benchmark tasks. This methodology provides a consistent framework for \\ninvestigating how well task-specific distillation allows compact models to inherit and preserve \\nthe teacher's semantic reasoning and language comprehension abilities. \\n \\nResults and Discussion \\n \\nAs shown in Table 1 and Table 2, t he results demonstrates that task -specific knowledge\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content=\"As shown in Table 1 and Table 2, t he results demonstrates that task -specific knowledge \\ndistillation improves the compact LLMs' Natural Language Understanding skills on all three \\ntasks.  In text classification, the teacher achieves a solid baseline of 0.5534 accuracy and 0.5668 \\nF1 score, however the  student performs poorly at 0.3100 accuracy.  After distillation, the \\nFigure 1: High level methodology diagram\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"3 \\n \\n \\nstudent improves to 0.4800 accuracy and 0.4952 F1, indicating that the teacher logits assist the \\nmodel in learning discriminative patterns and reducing misclassification.  Similarly, in \\nsentiment analysis, the teacher reaches 0.6550 accuracy and 0.6318 F1,  whereas student \\ninitially achieves 0.4025 accuracy.  Distillation improves the student's accuracy and F1 to \\n0.5900 and 0.5628  respectively, indicating increased detection of subtle sentiment clues and \\nless polarity errors. \\n \\nTable 1: Text classification and sentiment analysis performance \\nTask Model Accuracy Precision Recall F1 Score \\nText \\nClassification \\nTeacher (LLaMA \\n3.1:70B) \\n0.5534 0.5111 0.5801 0.5668 \\nStudent (LLaMA \\n3.1:8B) \\n0.3100 0.2900 0.2101 0.3552 \\nDistilled 0.4800 0.4700 0.4300 0.4952 \\nSentiment \\nAnalysis \\nTeacher (LLaMA \\n3.1:70B) \\n0.6550 0.6210 0.6430 0.6318 \\nStudent (LLaMA \\n3.1:8B) \\n0.4025 0.3901 0.3620 0.3756 \\nDistilled 0.5900 0.5740 0.5520 0.5628 \\n \\n \\nTable 2: Text summarization task performance\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"Distilled 0.5900 0.5740 0.5520 0.5628 \\n \\n \\nTable 2: Text summarization task performance \\nModel STS-B (Pearson \\nCorr.) \\nMNLI Accuracy Average Response \\ntime (s) \\nTeacher (LLaMA \\n3.1:70B) \\n0.94 0.90 12.0 \\nStudent (LLaMA \\n3.1:8B) \\n0.58 0.65 3.2 \\nDistilled 0.85 0.81 3.0 \\n \\nThe teacher obtains 0.94 STS -B and 0.90 MNLI for text summarization, but the student \\nperforms much lower at 0.58 STS-B and 0.65 MNLI.  After distillation, the student advances \\nto 0.85 STS-B and 0.81 MNLI, recovering more than 90% of the teacher's performance.  The \\ndistilled model retains low inference latency, providing a 4x speedup and creating more truthful \\nand coherent summaries.  Overall, the findings show that task-specific logit-based knowledge \\ndistillation improves compact LLMs' semantic reasoning, class ification stability, and \\ncontextual comprehension.  The distilled model consistently outperforms the baseline student\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='contextual comprehension.  The distilled model consistently outperforms the baseline student \\nacross all tasks and exhibits teacher -like NLU behavior while maintaining computational \\nefficiency, making it appropriate for real-world, resource-constrained implementation. \\n \\nConclusions \\n \\nThis study demonstrates that task -specific, logit -based knowledge distillation significantly'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content=\"4 \\n \\n \\nimproves NLU in compact LLMs.  By transferring soft -label predictions and task -specific \\nreasoning from LLaMA3.1:70B to LLaMA3.1:8B, the distilled model significantly  improves \\ntext classification, sentiment analysis, and summarization.  It captures deeper semantic links, \\ncontextual reasoning, and classification consistency, restoring much of the teacher's \\nperformance while minimizing inference delay.  These findings sho w that task -focused \\ndistillation allows compact models to efficiently accomplish teacher-like NLU behavior, hence \\npromoting high-quality language understanding in resource-constrained environments. \\n \\nAlthough the results are favorable, the study has a few drawbacks.  Due to limited computing \\npower, substantial fine -tuning, domain adaptation, and repeat experimental runs were not \\npossible.  The evaluation was based on a limited collection of datasets and benchmarks, which\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content=\"possible.  The evaluation was based on a limited collection of datasets and benchmarks, which \\nmay not apply to larger domains, languages, or real-world settings.  Furthermore, the distillation \\nprocess only exploited logit -level transfer, leaving deeper internal reasoning processes \\nuntapped.  To increase its versatility and application, future work might broaden this framework \\nto include multi-class, multilingual, and cross-domain scenarios. Further improvements could \\nbe achieved by investigating deeper distillation approaches, including as layer -wise or \\nrepresentation-level trans fer, as well as fine -tuning the instructor for specific tasks prior to \\ndistillation.  Complementary studies on interpretability and error analysis might give more \\ninformation about how well the student model recalls and applies the teacher's reasoning. \\n \\nIn conclusion, this research demonstrates that task-specific, logit-based knowledge distillation\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='In conclusion, this research demonstrates that task-specific, logit-based knowledge distillation \\nis a useful technique for improving NLU in compact LLMs.  By demonstrating consistent \\nperformance improvements across multiple tasks, the research indicates th e potential of \\ntargeted distillation approaches for  deploying high -performing, resource -efficient LLMs in \\nreal-world applications, providing the groundwork for future advances in compact NLU \\nmodels. \\n    \\nReferences \\n \\nBalasubramaniam, G., Abishethvarman, V., Kumara, B. T. G. S., Prasanth, S., & \\nKuhaneswaran, B. (2026). Task -Specific Knowledge Distillation for  Scalable \\nSentiment Classification in  Low-Resource Settings. In C. Anutariya, M. Bonsangue, \\nA. Pinidiyaarachchi, & H. Usoof, Data Science and Artificial Intelligence Singapore. \\nGou, J., Yu, B., Maybank, S. J., & Tao, D. (2021). Knowledge distillation: A survey. \\nInternational Journal of Computer Vision, 129(6), 1789-1819.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-12-10T22:41:52+05:30', 'author': 'SYMPO', 'moddate': '2025-12-10T22:41:52+05:30', 'source': './Docs/COMURS_GB_v4.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='International Journal of Computer Vision, 129(6), 1789-1819.  \\nJiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L., Wang, F., & Liu, Q. (2019). Tinybert: \\nDistilling bert for natural language understanding. arXiv preprint arXiv:1909. 10351.  \\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of \\nBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.  \\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y., & Zhou, D. (2020). Mobilebert: a compact task -\\nagnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c207d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b2cdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m      4\u001b[39m embedding_model = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m vectorstore = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:709\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Unconditionally call _get_len_safe_embeddings to handle length safety.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This could be optimized to avoid double work when all texts are short enough.\u001b[39;00m\n\u001b[32m    708\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:576\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;66;03m# Make API call with this batch\u001b[39;00m\n\u001b[32m    575\u001b[39m batch_tokens = tokens[i:batch_end]\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    578\u001b[39m     response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG-yt\\venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model, persist_directory=\"./chroma_db/\", collection_name=\"comurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"What is NLU? Tell me in one line\", k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c9dd5",
   "metadata": {},
   "source": [
    "## Use vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_persist = Chroma(persist_directory=\"./chroma_db/\", \n",
    "                             embedding_function=embedding_model, \n",
    "                             collection_name=\"comurs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788040c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_persist.similarity_search(\"What is NLU? Tell me in one line\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a7858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c497db41",
   "metadata": {},
   "source": [
    "### Store embeddings in the existing local vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(persist_directory=\"./chroma_db/\", \n",
    "                             embedding_function=embedding_model)\n",
    "\n",
    "\n",
    "vectorstore.add_documents(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
